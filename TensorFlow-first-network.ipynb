{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our first neural network in tensorflow\n",
    "\n",
    "In this tutorial part we will build a deep neural network using tensorflow. \n",
    "\n",
    "- Create the computation graph\n",
    "- Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 3\n",
    "CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prepare_data(images):\n",
    "    m = len(images)\n",
    "    X = np.zeros((m, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "    y = np.zeros((1, m), dtype=np.uint8)\n",
    "    for i, image_file in enumerate(images):\n",
    "        X[i,:] = read_image(image_file)\n",
    "        if 'dog' in image_file.lower():\n",
    "            y[0, i] = 1\n",
    "        elif 'cat' in image_file.lower():\n",
    "            y[0, i] = 0\n",
    "    return X, y\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'Train_data/'\n",
    "TEST_DIR = 'Test_data/'\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)]\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "train_set_x, train_set_y = prepare_data(train_images)\n",
    "test_set_x, test_set_y = prepare_data(test_images)\n",
    "\n",
    "train_set_x_flatten = train_set_x.reshape(train_set_x.shape[0], ROWS*COLS*CHANNELS).T\n",
    "test_set_x_flatten = test_set_x.reshape(test_set_x.shape[0], -1).T\n",
    "\n",
    "X_train = train_set_x_flatten/255\n",
    "X_test = test_set_x_flatten/255\n",
    "\n",
    "Y_train = convert_to_one_hot(train_set_y, CLASSES)\n",
    "Y_test = convert_to_one_hot(test_set_y, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we flatten the image dataset, then normalize it by dividing by 255. On top of that, we will convert each label to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 6002\n",
      "number of test examples = 1000\n",
      "X_train shape: (12288, 6002)\n",
      "Y_train shape: (2, 6002)\n",
      "X_test shape: (12288, 1000)\n",
      "Y_test shape: (2, 1000)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples =\", X_train.shape[1])\n",
    "print (\"number of test examples =\", X_test.shape[1])\n",
    "print (\"X_train shape:\", X_train.shape)\n",
    "print (\"Y_train shape:\", Y_train.shape)\n",
    "print (\"X_test shape:\", X_test.shape)\n",
    "print (\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal** is to build an algorithm capable of recognizing a cat and dog with high accuracy. To do so, we are going to build a tensorflow model that is almost the same as one we have previously built in numpy for cat recognition (but now using a softmax output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create placeholders\n",
    "\n",
    "Our first task is to create placeholders for `X` and `Y`. \n",
    "\n",
    "**Arguments**:<br>\n",
    "n_x - scalar, size of an image vector (num_px * num_px = ROWS * COLS * CHANNELS = 12288)<br>\n",
    "n_y - scalar, number of classes (from 0 to 1, so -> 2)<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "X - placeholder for the data input, of shape [n_x, None] and dtype \"float\"<br>\n",
    "Y - placeholder for the input labels, of shape [n_y, None] and dtype \"float\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name = 'X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name = 'Y')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(X_train.shape[0], CLASSES)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Initializing the parameters\n",
    "\n",
    "**Arguments**:<br>\n",
    "INPUT, h1, h2, OUTPUT - size of model layers<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "parameters - a dictionary of tensors containing W1, b1, W2, b2, W3, b3<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(INPUT, h1, h2, OUTPUT):\n",
    "    W1 = tf.get_variable(\"W1\", [h1, INPUT], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", [h1, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [h2, h1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [h2, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [OUTPUT, h2], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", [OUTPUT, 1], initializer = tf.zeros_initializer())\n",
    " \n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =  <tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref>\n",
      "b1 =  <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>\n",
      "W2 =  <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>\n",
      "b2 =  <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>\n",
      "W3 =  <tf.Variable 'W3:0' shape=(2, 12) dtype=float32_ref>\n",
      "b3 =  <tf.Variable 'b3:0' shape=(2, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(X_train.shape[0], 25, 12, CLASSES)\n",
    "    print(\"W1 = \", parameters[\"W1\"])\n",
    "    print(\"b1 = \", parameters[\"b1\"])\n",
    "    print(\"W2 = \", parameters[\"W2\"])\n",
    "    print(\"b2 = \", parameters[\"b2\"])\n",
    "    print(\"W3 = \", parameters[\"W3\"])\n",
    "    print(\"b3 = \", parameters[\"b3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Forward propagation in tensorflow \n",
    "\n",
    "The functions we will be using are: \n",
    "\n",
    "- `tf.add(...,...)` to do an addition\n",
    "- `tf.matmul(...,...)` to do a matrix multiplication\n",
    "- `tf.nn.relu(...)` to apply the ReLU activation\n",
    "\n",
    "**Arguments**:<br>\n",
    "X - input dataset placeholder, of shape (input size, number of examples)<br>\n",
    "parameters - python dictionary containing our parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\" the shapes are given in initialize_parameters<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "Z3 - the output of the last LINEAR unit<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieving parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "                                      # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)   # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)               # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)  # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)               # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)  # Z3 = np.dot(W3,Z2) + b3\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"Add_2:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], CLASSES)\n",
    "    parameters = initialize_parameters(X_train.shape[0], 25, 12, CLASSES)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 =\", Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Compute cost\n",
    "\n",
    "As seen before, it is very easy to compute the cost using:\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))\n",
    "```\n",
    "**Note**: \n",
    "- It is important to know that the \"`logits`\" and \"`labels`\" inputs of `tf.nn.softmax_cross_entropy_with_logits` are expected to be of shape (number of examples, num_classes) so I have transposed Z3 and Y for you.\n",
    "- Besides, `tf.reduce_mean` basically does the summation over the examples.\n",
    "\n",
    "**Arguments**:<br>\n",
    "Z3 - output of forward propagation (output of the last LINEAR unit), of shape (CLASSES, number of examples)<br>\n",
    "Y - \"true\" labels vector placeholder, same shape as Z3<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "cost - Tensor of the cost function<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):   \n",
    "    # fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], CLASSES)\n",
    "    parameters = initialize_parameters(X_train.shape[0], 25, 12, CLASSES)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost =\",cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Mini-Batch Gradient descent\n",
    "\n",
    "Let's build mini-batches from the training set (X, Y) in two steps:\n",
    "\n",
    "- **Shuffle**: \n",
    "\n",
    "<img src=\"images/shuffle.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "- **Partition**:\n",
    "\n",
    "<img src=\"images/partition.png\" style=\"width:550px;height:300px;\"> \n",
    "\n",
    "**Arguments**:<br>\n",
    "X - input data, of shape (input size, number of examples)<br>\n",
    "Y - true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)<br>\n",
    "mini_batch_size - size of the mini-batches, integer<br>\n",
    "seed - this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "mini_batches - list of synchronous (mini_batch_X, mini_batch_Y)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):   \n",
    "    # number of training examples\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (12288, 64)\n",
      "shape of the 1st mini_batch_Y: (2, 64)\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(X_train, Y_train, 64)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X:\", mini_batches[0][0].shape)\n",
    "print (\"shape of the 1st mini_batch_Y:\", mini_batches[0][1].shape)\n",
    "print(len(mini_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we should remember**:\n",
    "- Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "- Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation & parameter updates\n",
    "\n",
    "For instance, for gradient descent the optimizer would be:\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "```\n",
    "\n",
    "To make the optimization we would do:\n",
    "```python\n",
    "_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Building the model\n",
    "\n",
    "So we'll implement a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "\n",
    "**Arguments**:<br>\n",
    "    X_train - training set, of shape (input size, number of training examples)<br>\n",
    "    Y_train - test set, of shape (output size = CLASSES, number of training examples)<br>\n",
    "    X_test - training set, of shape (input size, number of training examples)<br>\n",
    "    Y_test - test set, of shape (output size = CLASSES, number of test examples)<br>\n",
    "    learning_rate - learning rate of the optimization<br>\n",
    "    num_epochs - number of epochs of the optimization loop<br>\n",
    "    minibatch_size - size of a minibatch<br>\n",
    "    print_cost - True to print the cost every 100 epochs<br>\n",
    "\n",
    "**Returns**:<br>\n",
    "    parameters - parameters learnt by the model. They can then be used to predict.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the current graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1000, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "\n",
    "    # (n_x: input size, m : number of examples in the train set)\n",
    "    (n_x, m) = X_train.shape                        \n",
    "    # n_y : output size\n",
    "    n_y = Y_train.shape[0]                        \n",
    "    # To keep track of the cost\n",
    "    costs = []                                        \n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(X_train.shape[0], 200, 20, CLASSES)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        # for epoch in range(num_epochs): #Remove problem for loop\n",
    "        epoch = 0                   #My While loop setup\n",
    "        while epoch < num_epochs:   #My While loop setup\n",
    "            epoch = epoch + 1       #My While loop setup\n",
    "            epoch_cost = 0.         # Defines a cost related to an epoch\n",
    "            # number of minibatches of size minibatch_size in the train set\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            # seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        # Save our trained model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './TensorFlow-first-network')\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train our model. On my machine it takes about 15 minutes, with GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 100: 0.149014\n",
      "Cost after epoch 200: 0.007334\n",
      "Cost after epoch 300: 0.002902\n",
      "Cost after epoch 400: 0.000667\n",
      "Cost after epoch 500: 0.004335\n",
      "Cost after epoch 600: 0.001176\n",
      "Cost after epoch 700: 0.001851\n",
      "Cost after epoch 800: 0.000289\n",
      "Cost after epoch 900: 0.001448\n",
      "Cost after epoch 1000: 0.000860\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.592\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Test with your own image\n",
    "\n",
    "We can now take a picture of our cat or dog and see the output of our model. To do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image = \"cat.jpg\"\n",
    "test_image = \"dog.jpg\"\n",
    "my_image = read_image(test_image).reshape(1, ROWS*COLS*CHANNELS).T\n",
    "X = my_image / 255.\n",
    "#print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we use predict function to predict Cat vs Dog from our X image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [X.shape[0], 1])\n",
    "    \n",
    "    z3 = forward_propagation(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return str(np.squeeze(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Restore the graph from .meta file\n",
    "\n",
    "When we save the variables, it creates a .meta file. This file contains the graph structure. Therefore, we can import the meta graph `using tf.train.import_meta_graph()` and restore the values of the graph. Let's import the graph and print all tensors in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Y\n",
      "W1/Initializer/random_uniform/shape\n",
      "W1/Initializer/random_uniform/min\n",
      "W1/Initializer/random_uniform/max\n",
      "W1/Initializer/random_uniform/RandomUniform\n",
      "W1/Initializer/random_uniform/sub\n",
      "W1/Initializer/random_uniform/mul\n",
      "W1/Initializer/random_uniform\n",
      "W1\n",
      "W1/Assign\n",
      "W1/read\n",
      "b1/Initializer/zeros\n",
      "b1\n",
      "b1/Assign\n",
      "b1/read\n",
      "W2/Initializer/random_uniform/shape\n",
      "W2/Initializer/random_uniform/min\n",
      "W2/Initializer/random_uniform/max\n",
      "W2/Initializer/random_uniform/RandomUniform\n",
      "W2/Initializer/random_uniform/sub\n",
      "W2/Initializer/random_uniform/mul\n",
      "W2/Initializer/random_uniform\n",
      "W2\n",
      "W2/Assign\n",
      "W2/read\n",
      "b2/Initializer/zeros\n",
      "b2\n",
      "b2/Assign\n",
      "b2/read\n",
      "W3/Initializer/random_uniform/shape\n",
      "W3/Initializer/random_uniform/min\n",
      "W3/Initializer/random_uniform/max\n",
      "W3/Initializer/random_uniform/RandomUniform\n",
      "W3/Initializer/random_uniform/sub\n",
      "W3/Initializer/random_uniform/mul\n",
      "W3/Initializer/random_uniform\n",
      "W3\n",
      "W3/Assign\n",
      "W3/read\n",
      "b3/Initializer/zeros\n",
      "b3\n",
      "b3/Assign\n",
      "b3/read\n",
      "MatMul\n",
      "Add\n",
      "Relu\n",
      "MatMul_1\n",
      "Add_1\n",
      "Relu_1\n",
      "MatMul_2\n",
      "Add_2\n",
      "transpose/perm\n",
      "transpose\n",
      "transpose_1/perm\n",
      "transpose_1\n",
      "softmax_cross_entropy_with_logits_sg/labels_stop_gradient\n",
      "softmax_cross_entropy_with_logits_sg/Rank\n",
      "softmax_cross_entropy_with_logits_sg/Shape\n",
      "softmax_cross_entropy_with_logits_sg/Rank_1\n",
      "softmax_cross_entropy_with_logits_sg/Shape_1\n",
      "softmax_cross_entropy_with_logits_sg/Sub/y\n",
      "softmax_cross_entropy_with_logits_sg/Sub\n",
      "softmax_cross_entropy_with_logits_sg/Slice/begin\n",
      "softmax_cross_entropy_with_logits_sg/Slice/size\n",
      "softmax_cross_entropy_with_logits_sg/Slice\n",
      "softmax_cross_entropy_with_logits_sg/concat/values_0\n",
      "softmax_cross_entropy_with_logits_sg/concat/axis\n",
      "softmax_cross_entropy_with_logits_sg/concat\n",
      "softmax_cross_entropy_with_logits_sg/Reshape\n",
      "softmax_cross_entropy_with_logits_sg/Rank_2\n",
      "softmax_cross_entropy_with_logits_sg/Shape_2\n",
      "softmax_cross_entropy_with_logits_sg/Sub_1/y\n",
      "softmax_cross_entropy_with_logits_sg/Sub_1\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1/begin\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1/size\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1\n",
      "softmax_cross_entropy_with_logits_sg/concat_1/values_0\n",
      "softmax_cross_entropy_with_logits_sg/concat_1/axis\n",
      "softmax_cross_entropy_with_logits_sg/concat_1\n",
      "softmax_cross_entropy_with_logits_sg/Reshape_1\n",
      "softmax_cross_entropy_with_logits_sg\n",
      "softmax_cross_entropy_with_logits_sg/Sub_2/y\n",
      "softmax_cross_entropy_with_logits_sg/Sub_2\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2/begin\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2/size\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2\n",
      "softmax_cross_entropy_with_logits_sg/Reshape_2\n",
      "Const\n",
      "Mean\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/Mean_grad/Reshape/shape\n",
      "gradients/Mean_grad/Reshape\n",
      "gradients/Mean_grad/Shape\n",
      "gradients/Mean_grad/Tile\n",
      "gradients/Mean_grad/Shape_1\n",
      "gradients/Mean_grad/Shape_2\n",
      "gradients/Mean_grad/Const\n",
      "gradients/Mean_grad/Prod\n",
      "gradients/Mean_grad/Const_1\n",
      "gradients/Mean_grad/Prod_1\n",
      "gradients/Mean_grad/Maximum/y\n",
      "gradients/Mean_grad/Maximum\n",
      "gradients/Mean_grad/floordiv\n",
      "gradients/Mean_grad/Cast\n",
      "gradients/Mean_grad/truediv\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Shape\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Reshape\n",
      "gradients/zeros_like\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/mul\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/Neg\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/group_deps\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape\n",
      "gradients/transpose_grad/InvertPermutation\n",
      "gradients/transpose_grad/transpose\n",
      "gradients/Add_2_grad/Shape\n",
      "gradients/Add_2_grad/Shape_1\n",
      "gradients/Add_2_grad/BroadcastGradientArgs\n",
      "gradients/Add_2_grad/Sum\n",
      "gradients/Add_2_grad/Reshape\n",
      "gradients/Add_2_grad/Sum_1\n",
      "gradients/Add_2_grad/Reshape_1\n",
      "gradients/Add_2_grad/tuple/group_deps\n",
      "gradients/Add_2_grad/tuple/control_dependency\n",
      "gradients/Add_2_grad/tuple/control_dependency_1\n",
      "gradients/MatMul_2_grad/MatMul\n",
      "gradients/MatMul_2_grad/MatMul_1\n",
      "gradients/MatMul_2_grad/tuple/group_deps\n",
      "gradients/MatMul_2_grad/tuple/control_dependency\n",
      "gradients/MatMul_2_grad/tuple/control_dependency_1\n",
      "gradients/Relu_1_grad/ReluGrad\n",
      "gradients/Add_1_grad/Shape\n",
      "gradients/Add_1_grad/Shape_1\n",
      "gradients/Add_1_grad/BroadcastGradientArgs\n",
      "gradients/Add_1_grad/Sum\n",
      "gradients/Add_1_grad/Reshape\n",
      "gradients/Add_1_grad/Sum_1\n",
      "gradients/Add_1_grad/Reshape_1\n",
      "gradients/Add_1_grad/tuple/group_deps\n",
      "gradients/Add_1_grad/tuple/control_dependency\n",
      "gradients/Add_1_grad/tuple/control_dependency_1\n",
      "gradients/MatMul_1_grad/MatMul\n",
      "gradients/MatMul_1_grad/MatMul_1\n",
      "gradients/MatMul_1_grad/tuple/group_deps\n",
      "gradients/MatMul_1_grad/tuple/control_dependency\n",
      "gradients/MatMul_1_grad/tuple/control_dependency_1\n",
      "gradients/Relu_grad/ReluGrad\n",
      "gradients/Add_grad/Shape\n",
      "gradients/Add_grad/Shape_1\n",
      "gradients/Add_grad/BroadcastGradientArgs\n",
      "gradients/Add_grad/Sum\n",
      "gradients/Add_grad/Reshape\n",
      "gradients/Add_grad/Sum_1\n",
      "gradients/Add_grad/Reshape_1\n",
      "gradients/Add_grad/tuple/group_deps\n",
      "gradients/Add_grad/tuple/control_dependency\n",
      "gradients/Add_grad/tuple/control_dependency_1\n",
      "gradients/MatMul_grad/MatMul\n",
      "gradients/MatMul_grad/MatMul_1\n",
      "gradients/MatMul_grad/tuple/group_deps\n",
      "gradients/MatMul_grad/tuple/control_dependency\n",
      "gradients/MatMul_grad/tuple/control_dependency_1\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "W1/Adam/Initializer/zeros/shape_as_tensor\n",
      "W1/Adam/Initializer/zeros/Const\n",
      "W1/Adam/Initializer/zeros\n",
      "W1/Adam\n",
      "W1/Adam/Assign\n",
      "W1/Adam/read\n",
      "W1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "W1/Adam_1/Initializer/zeros/Const\n",
      "W1/Adam_1/Initializer/zeros\n",
      "W1/Adam_1\n",
      "W1/Adam_1/Assign\n",
      "W1/Adam_1/read\n",
      "b1/Adam/Initializer/zeros\n",
      "b1/Adam\n",
      "b1/Adam/Assign\n",
      "b1/Adam/read\n",
      "b1/Adam_1/Initializer/zeros\n",
      "b1/Adam_1\n",
      "b1/Adam_1/Assign\n",
      "b1/Adam_1/read\n",
      "W2/Adam/Initializer/zeros/shape_as_tensor\n",
      "W2/Adam/Initializer/zeros/Const\n",
      "W2/Adam/Initializer/zeros\n",
      "W2/Adam\n",
      "W2/Adam/Assign\n",
      "W2/Adam/read\n",
      "W2/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "W2/Adam_1/Initializer/zeros/Const\n",
      "W2/Adam_1/Initializer/zeros\n",
      "W2/Adam_1\n",
      "W2/Adam_1/Assign\n",
      "W2/Adam_1/read\n",
      "b2/Adam/Initializer/zeros\n",
      "b2/Adam\n",
      "b2/Adam/Assign\n",
      "b2/Adam/read\n",
      "b2/Adam_1/Initializer/zeros\n",
      "b2/Adam_1\n",
      "b2/Adam_1/Assign\n",
      "b2/Adam_1/read\n",
      "W3/Adam/Initializer/zeros\n",
      "W3/Adam\n",
      "W3/Adam/Assign\n",
      "W3/Adam/read\n",
      "W3/Adam_1/Initializer/zeros\n",
      "W3/Adam_1\n",
      "W3/Adam_1/Assign\n",
      "W3/Adam_1/read\n",
      "b3/Adam/Initializer/zeros\n",
      "b3/Adam\n",
      "b3/Adam/Assign\n",
      "b3/Adam/read\n",
      "b3/Adam_1/Initializer/zeros\n",
      "b3/Adam_1\n",
      "b3/Adam_1/Assign\n",
      "b3/Adam_1/read\n",
      "Adam/learning_rate\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_W1/ApplyAdam\n",
      "Adam/update_b1/ApplyAdam\n",
      "Adam/update_W2/ApplyAdam\n",
      "Adam/update_b2/ApplyAdam\n",
      "Adam/update_W3/ApplyAdam\n",
      "Adam/update_b3/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "init\n",
      "ArgMax/dimension\n",
      "ArgMax\n",
      "ArgMax_1/dimension\n",
      "ArgMax_1\n",
      "Equal\n",
      "Cast\n",
      "Const_1\n",
      "Mean_1\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/Assign_12\n",
      "save/Assign_13\n",
      "save/Assign_14\n",
      "save/Assign_15\n",
      "save/Assign_16\n",
      "save/Assign_17\n",
      "save/Assign_18\n",
      "save/Assign_19\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# import the graph from the file\n",
    "imported_graph = tf.train.import_meta_graph('TensorFlow-first-network.meta')\n",
    "\n",
    "# list all the tensors in the graph\n",
    "for tensor in tf.get_default_graph().get_operations():\n",
    "    print (tensor.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.train.Saver()` saves the variables with the TensorFlow name. Now that we have the imported graph, we know that we are interested in W1, W2, W3 and b1, b2, b3 tensors, we can restore them from parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./TensorFlow-first-network\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    ## Load the entire model previuosly saved in a checkpoint\n",
    "    the_Saver = tf.train.import_meta_graph('TensorFlow-first-network' + '.meta')\n",
    "    the_Saver.restore(sess, './TensorFlow-first-network')\n",
    "\n",
    "    W1,b1,W2,b2,W3,b3 = sess.run(['W1:0', 'b1:0','W2:0','b2:0', 'W3:0','b3:0'])\n",
    "    parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    #print(\"W1 = \", parameters[\"W1\"])\n",
    "    #print(\"b1 = \", parameters[\"b1\"])\n",
    "    #print(\"W2 = \", parameters[\"W2\"])\n",
    "    #print(\"b2 = \", parameters[\"b2\"])\n",
    "    #print(\"W3 = \", parameters[\"W3\"])\n",
    "    #print(\"b3 = \", parameters[\"b3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
